{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import datasets\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GPT-2 tokenizer and model\n",
    "tokenizer = transformers.GPT2Tokenizer.from_pretrained('gpt2', device_map='auto')\n",
    "# Add a padding token to GPT-2 tokenizer (since it doesn't have one by default)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a small subset of the IMDb dataset for binary sentiment classification\n",
    "dataset = datasets.load_dataset('imdb', split='train[:50%]')\n",
    "dataset = dataset.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24a1bc6c7e514c6dac823809e1f090b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbda373828084d48aed59bbdc2004ba0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Annotate with whether a text contains the word 'plot'\n",
    "def set_word_label(example):\n",
    "    if 'plot' in example['text'].lower():\n",
    "        example['label'] = 1\n",
    "    else:\n",
    "        example['label'] = 0\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(set_word_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92d54033e7d8457c8110d683cee4b542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b56fb62d90e04bfdaec8c7633afddf48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'], padding='max_length', truncation=True, max_length=128\n",
    "    )\n",
    "\n",
    "# Apply the tokenization\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Remove unnecessary columns and set format for PyTorch\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['text'])\n",
    "tokenized_datasets.set_format('torch')\n",
    "\n",
    "# Split into train and evaluation datasets\n",
    "train_dataset = tokenized_datasets['train']\n",
    "eval_dataset = tokenized_datasets['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom model with GPT-2 as feature extractor and a linear classifier on top\n",
    "class GPT2ForClassification(torch.nn.Module):\n",
    "    def __init__(self, gpt2, num_labels):\n",
    "        super(GPT2ForClassification, self).__init__()\n",
    "        self.gpt2 = gpt2\n",
    "        # Freeze GPT-2 parameters\n",
    "        for param in self.gpt2.parameters():\n",
    "            param.requires_grad = False\n",
    "        # Linear classifier\n",
    "        self.classifier = torch.nn.Linear(self.gpt2.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        # Get hidden states from GPT-2\n",
    "        outputs = self.gpt2(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Use the hidden state of the last token for classification\n",
    "        last_token_indices = attention_mask.sum(dim=1) - 1\n",
    "        pooled_output = outputs.last_hidden_state[torch.arange(input_ids.size(0)), last_token_indices]\n",
    "        logits = self.classifier(pooled_output)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Compute loss\n",
    "            loss_fct = torch.nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.classifier.out_features), labels.view(-1))\n",
    "        return {'loss': loss, 'logits': logits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GPT-2\n",
    "gpt2 = transformers.GPT2Model.from_pretrained('gpt2', device_map='auto', torch_dtype='auto')\n",
    "\n",
    "# Initialize the model\n",
    "num_labels = 2  # Binary classification\n",
    "model = GPT2ForClassification(gpt2, num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments to train for only 1 epoch\n",
    "training_args = transformers.TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    eval_strategy=\"epoch\",\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute evaluation metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = torch.argmax(torch.from_numpy(logits), dim=-1).numpy()\n",
    "    accuracy = sklearn.metrics.accuracy_score(labels, predictions)\n",
    "    return {'accuracy': accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Trainer\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f66e5b3464954c738f5c9fc4a92eb401",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9479, 'grad_norm': 159.72096252441406, 'learning_rate': 4.92e-05, 'epoch': 0.02}\n",
      "{'loss': 0.76, 'grad_norm': 28.952970504760742, 'learning_rate': 4.8400000000000004e-05, 'epoch': 0.03}\n",
      "{'loss': 0.6734, 'grad_norm': 72.84471893310547, 'learning_rate': 4.76e-05, 'epoch': 0.05}\n",
      "{'loss': 0.6677, 'grad_norm': 12.976938247680664, 'learning_rate': 4.6800000000000006e-05, 'epoch': 0.06}\n",
      "{'loss': 0.6209, 'grad_norm': 31.5085391998291, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.08}\n",
      "{'loss': 0.6092, 'grad_norm': 27.92189598083496, 'learning_rate': 4.52e-05, 'epoch': 0.1}\n",
      "{'loss': 0.6358, 'grad_norm': 11.485227584838867, 'learning_rate': 4.44e-05, 'epoch': 0.11}\n",
      "{'loss': 0.7262, 'grad_norm': 82.11431884765625, 'learning_rate': 4.36e-05, 'epoch': 0.13}\n",
      "{'loss': 0.626, 'grad_norm': 68.08614349365234, 'learning_rate': 4.2800000000000004e-05, 'epoch': 0.14}\n",
      "{'loss': 0.6629, 'grad_norm': 31.249441146850586, 'learning_rate': 4.2e-05, 'epoch': 0.16}\n",
      "{'loss': 0.6413, 'grad_norm': 27.914081573486328, 'learning_rate': 4.12e-05, 'epoch': 0.18}\n",
      "{'loss': 0.6087, 'grad_norm': 47.73126220703125, 'learning_rate': 4.0400000000000006e-05, 'epoch': 0.19}\n",
      "{'loss': 0.6261, 'grad_norm': 32.310028076171875, 'learning_rate': 3.960000000000001e-05, 'epoch': 0.21}\n",
      "{'loss': 0.6048, 'grad_norm': 36.71888732910156, 'learning_rate': 3.88e-05, 'epoch': 0.22}\n",
      "{'loss': 0.7146, 'grad_norm': 11.359569549560547, 'learning_rate': 3.8e-05, 'epoch': 0.24}\n",
      "{'loss': 0.6476, 'grad_norm': 9.595714569091797, 'learning_rate': 3.72e-05, 'epoch': 0.26}\n",
      "{'loss': 0.6322, 'grad_norm': 99.89386749267578, 'learning_rate': 3.6400000000000004e-05, 'epoch': 0.27}\n",
      "{'loss': 0.561, 'grad_norm': 17.09288215637207, 'learning_rate': 3.56e-05, 'epoch': 0.29}\n",
      "{'loss': 0.7189, 'grad_norm': 18.85990333557129, 'learning_rate': 3.48e-05, 'epoch': 0.3}\n",
      "{'loss': 0.5755, 'grad_norm': 8.719954490661621, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.32}\n",
      "{'loss': 0.7114, 'grad_norm': 44.69769287109375, 'learning_rate': 3.32e-05, 'epoch': 0.34}\n",
      "{'loss': 0.6431, 'grad_norm': 100.55694580078125, 'learning_rate': 3.24e-05, 'epoch': 0.35}\n",
      "{'loss': 0.6445, 'grad_norm': 47.62276840209961, 'learning_rate': 3.16e-05, 'epoch': 0.37}\n",
      "{'loss': 0.7025, 'grad_norm': 22.08722496032715, 'learning_rate': 3.08e-05, 'epoch': 0.38}\n",
      "{'loss': 0.6738, 'grad_norm': 36.13248062133789, 'learning_rate': 3e-05, 'epoch': 0.4}\n",
      "{'loss': 0.66, 'grad_norm': 23.63441276550293, 'learning_rate': 2.9199999999999998e-05, 'epoch': 0.42}\n",
      "{'loss': 0.6709, 'grad_norm': 48.5285758972168, 'learning_rate': 2.84e-05, 'epoch': 0.43}\n",
      "{'loss': 0.5953, 'grad_norm': 51.18750762939453, 'learning_rate': 2.7600000000000003e-05, 'epoch': 0.45}\n",
      "{'loss': 0.7225, 'grad_norm': 41.909114837646484, 'learning_rate': 2.6800000000000004e-05, 'epoch': 0.46}\n",
      "{'loss': 0.6476, 'grad_norm': 21.056299209594727, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.48}\n",
      "{'loss': 0.6092, 'grad_norm': 41.71092224121094, 'learning_rate': 2.5200000000000003e-05, 'epoch': 0.5}\n",
      "{'loss': 0.662, 'grad_norm': 20.268814086914062, 'learning_rate': 2.44e-05, 'epoch': 0.51}\n",
      "{'loss': 0.6623, 'grad_norm': 52.49138641357422, 'learning_rate': 2.36e-05, 'epoch': 0.53}\n",
      "{'loss': 0.6364, 'grad_norm': 75.9678726196289, 'learning_rate': 2.2800000000000002e-05, 'epoch': 0.54}\n",
      "{'loss': 0.5839, 'grad_norm': 35.45322799682617, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.56}\n",
      "{'loss': 0.674, 'grad_norm': 34.080875396728516, 'learning_rate': 2.12e-05, 'epoch': 0.58}\n",
      "{'loss': 0.664, 'grad_norm': 68.73213958740234, 'learning_rate': 2.04e-05, 'epoch': 0.59}\n",
      "{'loss': 0.665, 'grad_norm': 16.472007751464844, 'learning_rate': 1.9600000000000002e-05, 'epoch': 0.61}\n",
      "{'loss': 0.6037, 'grad_norm': 10.664989471435547, 'learning_rate': 1.88e-05, 'epoch': 0.62}\n",
      "{'loss': 0.7079, 'grad_norm': 63.17946243286133, 'learning_rate': 1.8e-05, 'epoch': 0.64}\n",
      "{'loss': 0.585, 'grad_norm': 23.1137752532959, 'learning_rate': 1.7199999999999998e-05, 'epoch': 0.66}\n",
      "{'loss': 0.556, 'grad_norm': 61.788978576660156, 'learning_rate': 1.6400000000000002e-05, 'epoch': 0.67}\n",
      "{'loss': 0.6803, 'grad_norm': 127.72013092041016, 'learning_rate': 1.56e-05, 'epoch': 0.69}\n",
      "{'loss': 0.7294, 'grad_norm': 48.25639724731445, 'learning_rate': 1.48e-05, 'epoch': 0.7}\n",
      "{'loss': 0.6197, 'grad_norm': 9.931629180908203, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.72}\n",
      "{'loss': 0.6292, 'grad_norm': 45.3941764831543, 'learning_rate': 1.32e-05, 'epoch': 0.74}\n",
      "{'loss': 0.6472, 'grad_norm': 27.513755798339844, 'learning_rate': 1.24e-05, 'epoch': 0.75}\n",
      "{'loss': 0.6271, 'grad_norm': 60.49140548706055, 'learning_rate': 1.16e-05, 'epoch': 0.77}\n",
      "{'loss': 0.6718, 'grad_norm': 19.17105484008789, 'learning_rate': 1.08e-05, 'epoch': 0.78}\n",
      "{'loss': 0.6182, 'grad_norm': 28.73503875732422, 'learning_rate': 1e-05, 'epoch': 0.8}\n",
      "{'loss': 0.6301, 'grad_norm': 47.837284088134766, 'learning_rate': 9.2e-06, 'epoch': 0.82}\n",
      "{'loss': 0.64, 'grad_norm': 7.242997169494629, 'learning_rate': 8.400000000000001e-06, 'epoch': 0.83}\n",
      "{'loss': 0.6768, 'grad_norm': 78.0823745727539, 'learning_rate': 7.6e-06, 'epoch': 0.85}\n",
      "{'loss': 0.6122, 'grad_norm': 50.088409423828125, 'learning_rate': 6.800000000000001e-06, 'epoch': 0.86}\n",
      "{'loss': 0.6092, 'grad_norm': 26.155166625976562, 'learning_rate': 6e-06, 'epoch': 0.88}\n",
      "{'loss': 0.663, 'grad_norm': 10.59747314453125, 'learning_rate': 5.2e-06, 'epoch': 0.9}\n",
      "{'loss': 0.5876, 'grad_norm': 17.18408203125, 'learning_rate': 4.4e-06, 'epoch': 0.91}\n",
      "{'loss': 0.6523, 'grad_norm': 79.22252655029297, 'learning_rate': 3.6e-06, 'epoch': 0.93}\n",
      "{'loss': 0.6582, 'grad_norm': 50.42867660522461, 'learning_rate': 2.8000000000000003e-06, 'epoch': 0.94}\n",
      "{'loss': 0.7208, 'grad_norm': 43.24760818481445, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.96}\n",
      "{'loss': 0.6898, 'grad_norm': 17.703125, 'learning_rate': 1.2000000000000002e-06, 'epoch': 0.98}\n",
      "{'loss': 0.6018, 'grad_norm': 34.26729965209961, 'learning_rate': 4.0000000000000003e-07, 'epoch': 0.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b41f24c03a6475ea15e39a343830dee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6224797964096069, 'eval_accuracy': 0.7024, 'eval_runtime': 12.3521, 'eval_samples_per_second': 202.395, 'eval_steps_per_second': 12.71, 'epoch': 1.0}\n",
      "{'train_runtime': 62.5166, 'train_samples_per_second': 159.958, 'train_steps_per_second': 9.997, 'train_loss': 0.6528895858764648, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=625, training_loss=0.6528895858764648, metrics={'train_runtime': 62.5166, 'train_samples_per_second': 159.958, 'train_steps_per_second': 9.997, 'total_flos': 0.0, 'train_loss': 0.6528895858764648, 'epoch': 1.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f412a9964f824d98b7a82f23d54b290c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6224797964096069,\n",
       " 'eval_accuracy': 0.7024,\n",
       " 'eval_runtime': 12.2255,\n",
       " 'eval_samples_per_second': 204.491,\n",
       " 'eval_steps_per_second': 12.842,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7408ad11ee34d41aa266572b8018f8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use the trainer to make predictions on the evaluation dataset\n",
    "predictions_output = trainer.predict(eval_dataset)\n",
    "probabilities = torch.nn.functional.softmax(torch.tensor(predictions_output.predictions), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   1,    4,   20,   64,   76,   79,   88,   90,   97,  111,  115,\n",
       "         121,  141,  183,  198,  209,  218,  219,  275,  287,  294,  308,\n",
       "         336,  337,  373,  375,  380,  389,  394,  402,  408,  420,  423,\n",
       "         442,  456,  496,  515,  523,  545,  555,  564,  576,  593,  612,\n",
       "         618,  632,  646,  654,  666,  670,  711,  735,  736,  771,  797,\n",
       "         806,  814,  839,  852,  855,  874,  937,  938,  951,  957,  959,\n",
       "        1003, 1015, 1029, 1039, 1053, 1061, 1085, 1097, 1113, 1122, 1136,\n",
       "        1144, 1159, 1167, 1176, 1188, 1196, 1207, 1222, 1250, 1253, 1296,\n",
       "        1305, 1323, 1350, 1406, 1443, 1446, 1500, 1541, 1594, 1601, 1605,\n",
       "        1640, 1651, 1652, 1696, 1704, 1714, 1729, 1758, 1784, 1824, 1835,\n",
       "        1840, 1849, 1859, 1871, 1887, 1922, 1923, 1944, 1952, 1965, 1993,\n",
       "        1997, 2032, 2052, 2115, 2125, 2145, 2161, 2163, 2207, 2221, 2237,\n",
       "        2253, 2283, 2297, 2317, 2338, 2341, 2347, 2377, 2404, 2418, 2431,\n",
       "        2437, 2439, 2458, 2462, 2471, 2490]),)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_output.predictions.argmax(axis=-1).nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   0,    6,    9,   13,   16,   24,   26,   36,   37,   40,   41,\n",
       "          49,   55,   59,   61,   62,   72,   73,   79,   82,   87,   89,\n",
       "          92,   94,   96,  104,  106,  107,  108,  112,  116,  117,  120,\n",
       "         131,  132,  134,  142,  143,  149,  154,  155,  164,  168,  169,\n",
       "         171,  173,  186,  187,  188,  189,  199,  201,  204,  205,  211,\n",
       "         219,  222,  224,  226,  230,  231,  232,  234,  239,  240,  246,\n",
       "         247,  257,  258,  264,  274,  275,  277,  278,  281,  282,  289,\n",
       "         290,  291,  297,  298,  301,  303,  306,  307,  310,  311,  315,\n",
       "         317,  323,  324,  325,  329,  332,  336,  337,  338,  341,  350,\n",
       "         352,  354,  356,  357,  361,  366,  367,  369,  370,  371,  377,\n",
       "         380,  381,  382,  383,  392,  402,  403,  408,  410,  419,  420,\n",
       "         422,  429,  442,  443,  445,  451,  452,  453,  457,  458,  460,\n",
       "         461,  465,  467,  470,  475,  485,  487,  488,  493,  507,  509,\n",
       "         518,  520,  523,  528,  529,  531,  536,  539,  549,  552,  553,\n",
       "         555,  565,  570,  573,  578,  590,  595,  596,  597,  599,  601,\n",
       "         604,  608,  611,  614,  615,  616,  622,  625,  629,  630,  633,\n",
       "         639,  642,  643,  654,  655,  656,  658,  660,  661,  665,  666,\n",
       "         677,  681,  686,  687,  690,  694,  695,  697,  699,  704,  707,\n",
       "         709,  713,  717,  718,  720,  722,  727,  733,  735,  737,  740,\n",
       "         742,  745,  746,  747,  749,  759,  760,  766,  767,  772,  775,\n",
       "         777,  780,  786,  787,  789,  790,  792,  793,  814,  817,  820,\n",
       "         822,  823,  827,  831,  834,  839,  850,  851,  859,  867,  869,\n",
       "         870,  875,  880,  881,  882,  883,  886,  887,  890,  891,  892,\n",
       "         893,  894,  898,  900,  901,  907,  914,  915,  916,  917,  920,\n",
       "         921,  925,  926,  927,  928,  929,  933,  936,  937,  939,  946,\n",
       "         948,  949,  952,  959,  963,  969,  971,  974,  977,  984,  987,\n",
       "         988,  989,  993,  995, 1004, 1009, 1013, 1018, 1022, 1028, 1029,\n",
       "        1033, 1042, 1043, 1045, 1049, 1051, 1052, 1053, 1054, 1059, 1060,\n",
       "        1061, 1062, 1065, 1069, 1073, 1074, 1083, 1084, 1089, 1090, 1091,\n",
       "        1096, 1099, 1104, 1108, 1109, 1114, 1115, 1118, 1120, 1124, 1128,\n",
       "        1134, 1135, 1145, 1148, 1153, 1155, 1157, 1160, 1161, 1162, 1164,\n",
       "        1166, 1167, 1168, 1169, 1172, 1179, 1181, 1185, 1187, 1192, 1195,\n",
       "        1209, 1215, 1220, 1224, 1225, 1226, 1228, 1233, 1239, 1247, 1252,\n",
       "        1260, 1265, 1272, 1273, 1283, 1285, 1287, 1296, 1297, 1298, 1299,\n",
       "        1305, 1308, 1318, 1319, 1325, 1326, 1332, 1334, 1338, 1339, 1347,\n",
       "        1352, 1354, 1359, 1363, 1368, 1372, 1381, 1382, 1384, 1386, 1399,\n",
       "        1401, 1405, 1413, 1415, 1420, 1422, 1426, 1428, 1429, 1430, 1431,\n",
       "        1432, 1436, 1439, 1450, 1462, 1466, 1468, 1473, 1475, 1479, 1480,\n",
       "        1482, 1486, 1492, 1495, 1502, 1504, 1505, 1507, 1511, 1512, 1517,\n",
       "        1520, 1536, 1542, 1543, 1544, 1546, 1547, 1559, 1564, 1565, 1568,\n",
       "        1574, 1575, 1581, 1584, 1586, 1588, 1593, 1597, 1600, 1605, 1609,\n",
       "        1610, 1616, 1619, 1622, 1626, 1630, 1634, 1635, 1639, 1640, 1641,\n",
       "        1642, 1643, 1646, 1657, 1667, 1669, 1670, 1673, 1681, 1682, 1689,\n",
       "        1692, 1699, 1700, 1705, 1708, 1713, 1719, 1721, 1725, 1727, 1746,\n",
       "        1748, 1749, 1762, 1766, 1769, 1773, 1774, 1778, 1779, 1780, 1786,\n",
       "        1792, 1795, 1805, 1808, 1817, 1819, 1825, 1834, 1837, 1838, 1840,\n",
       "        1841, 1842, 1843, 1849, 1852, 1853, 1856, 1858, 1859, 1864, 1867,\n",
       "        1873, 1883, 1885, 1891, 1915, 1932, 1934, 1935, 1936, 1943, 1946,\n",
       "        1948, 1949, 1950, 1952, 1955, 1965, 1967, 1968, 1970, 1974, 1977,\n",
       "        1978, 1985, 1986, 1994, 1998, 2002, 2006, 2015, 2025, 2028, 2029,\n",
       "        2034, 2036, 2042, 2044, 2045, 2046, 2048, 2052, 2053, 2055, 2058,\n",
       "        2060, 2065, 2071, 2074, 2079, 2080, 2083, 2087, 2091, 2102, 2109,\n",
       "        2110, 2111, 2113, 2116, 2117, 2118, 2133, 2140, 2142, 2143, 2146,\n",
       "        2149, 2150, 2151, 2157, 2162, 2163, 2166, 2172, 2174, 2177, 2178,\n",
       "        2181, 2186, 2187, 2188, 2195, 2197, 2198, 2209, 2214, 2222, 2223,\n",
       "        2224, 2237, 2240, 2245, 2249, 2250, 2254, 2256, 2259, 2263, 2264,\n",
       "        2268, 2272, 2273, 2275, 2276, 2285, 2287, 2288, 2294, 2297, 2299,\n",
       "        2309, 2315, 2317, 2325, 2326, 2327, 2334, 2336, 2340, 2344, 2348,\n",
       "        2355, 2356, 2357, 2360, 2364, 2365, 2366, 2398, 2409, 2415, 2417,\n",
       "        2421, 2425, 2432, 2433, 2438, 2439, 2441, 2442, 2445, 2447, 2463,\n",
       "        2466, 2467, 2469, 2470, 2475, 2480, 2484, 2485, 2486, 2489, 2490,\n",
       "        2492, 2499]),)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_output.label_ids.nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(149)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_output.predictions.argmax(axis=-1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(673)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(dataset['test']['label']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted probability: 0.7796695232391357\n",
      "True label: 0\n",
      "Text: The American Humane Association, which is the source of the familiar disclaimer \"No animals were harmed...\" (the registered trademark of the AHA), began to monitor the use of animals in film production more than 60 years ago, after a blindfolded horse was forced to leap to its death from the top of a cliff for a shot in the film Jesse James (1939). Needless to say, the atrocious act kills the whole entertainment aspect of this film for me. I suppose one could say that at least the horse didn't die in vain, since it was the beginning of the public waking up to the callous and horrendous pain caused animals for the glory of movie making, but I can't help but feel that if the poor animal had a choice, this sure wouldn't have been the path he would have taken!\n",
      "--------------------------------------------------------------------------------\n",
      "Predicted probability: 0.7515990734100342\n",
      "True label: 0\n",
      "Text: The story concerns a genealogy researcher (Mel Harris) who is hired by her Estee Lauder-like cosmetic queen aunt. Her aunt (by marriage we are left to presume) is trying to track down her long lost family in Europe. All they have to go on is a photo of a young girl standing by an ornate music box. The researcher heads to Europe and conducts her search in places like Milan, Budapest, and Vienna. The scenery is the real thing and is actually shot on location (unlike a Murder, She Wrote where Jessica is supposed to be visiting a far-flung locale and Lansbury never left Burbank). Anyway, she meets a young man who is also searching to solve a family mystery of his own and they team up to track down clues and menace bad guys. The dialogue, particularly the romantic dialogue, is terrible. I watched this because of the scenery but the script was so bad that I stayed on just to see if it would get worse. It did. Acting was also off. I can see why Mel Harris's career never really took off after thirtysomething, but she is adequate (seems too old for her co-star though). But, the supporting players are straight out of the community playhouse. I also lost count of how many times they say \"Budapest\" to each other. Yes, it is pronounced Bood-a-phesht. We know, okay? I realized halfway into the film that this had to be one of those Harlequin movies and sure enough it is. Guess that says it all.\n",
      "--------------------------------------------------------------------------------\n",
      "Predicted probability: 0.7243520617485046\n",
      "True label: 1\n",
      "Text: Five Across the Eyes starts as five young teenage girls are driving home in time for their curfew, they stop off at a store & accidentally hit another car & decide to just drive off & leave it. Soon after the other car forces them to stop & a crazy woman with a shotgun gets out & shouts at them, makes them take their clothes off & makes them pee on them & then randomly drives off. Shaken & shocked the girls think their ordeal is over but the crazy woman comes back for seconds as she seems intent on killing the terrified girls who are lost & are low on gas...<br /><br />Produced & directed by Greg Swinson & Ryan Theissen with Swinson writing the thing & Theissen responsible for the cinematography & editing I have to say that Five Across the Eyes is easily one of the worst films I have ever seen if not the worst, I mean I'm struggling to think of a film I have seen that's worse. Now let me start off by saying that I am sure a lot of the film-making decision taken here were deliberate to try & provoke atmosphere, tension, realism & suspense but there is not one aspect of Five Across the Eyes that I didn't hate it to be honest it looks like a bad home video that has been put up on YouTube & even then it's still slightly embarrassing & a frankly worthless waste of 90 odd minutes of my time that I could have been doing something more entertaining & fun like pulling my fingernails out with pliers. The reviews on the web seem quite positive but on the IMDb (the amount of 1 Star comments is revealing & they can't all be wrong, right?) & it's message board which I think is much more of an indicator of what the average person thinks it's absolutely trashed by just about everyone & the phrase 'the worst film I have ever seen' is used a few times & to be fair most of these negative comments mention th same things & I have to agree with them. The story is terrible, alright I suspect it's meant to be minimalistic but this minimal? There's never any reason or explanation for the events that happen & it just feels totally random. It goes on for ages, the amount of plot here would struggle to fill a thirty minute made for telly program let along a full length feature. The dialogue is awful with these annoying girls who don't seem to have a brain cell between them taking about random stuff & screaming a lot. Oh god the screaming, there are seemingly endless scenes of these girls screaming or crying or whining which not only irritates & annoys & prevents any sane viewer feeling any sort of sympathy for them it also makes what they are trying to say almost impossible to hear properly. Then there's the real killer, the entire film is set & shot within the confines of a mini van, seriously the camera never leaves this car & as you can imagine it gets really boring, add that the low body count of just one person killed on screen & Five Across the Eyes is a film that I hated with a passion.<br /><br />On a technical level again I can see that the film-making style here was a deliberate choice but I have to be honest again & say Five Across the Eyes is the worst looking film I have ever seen. As a fan of film I like my films to look like proper films as it's a visual medium & I definitely don't want them to look worse than the average YouTube video or my home films shot on a camcorder while I was drunk. It really does look that amateurish & that bad, it's a complete eyesore & I hated every moment of every second of it. Just think The Blair Witch Project (1999) only ten times worse looking & sounding & you will be almost there. There are times during Five Across the Eyes when you literally can't tell what's going on or happening because of the camera-work & the almost pitch black & grainy contrast levels. The violence is tame too with a few splashes of blood & a stabbing at the end.<br /><br />Low budget doesn't even begin to describe Five Across the Eyes, with a supposed budget of about $4,000 this is easily one of the lowest budgeted films ever given a wide release. The two vans in the film were owned by members of the production & that's basically pretty much the entire budget right there, the locations. The acting is pretty bad by the main cast, I just hated all the fake put on crying & screaming that didn't convince at all but did irritate immensely.<br /><br />Five Across the Eyes will go down as one of the worst films I have ever seen & I have seen a few films, whenever anyone now ask's me what's the worst film I have ever seen Five Across the Eyes will definitely get a mention. I hated it, every single aspect & wretched moment of it.\n",
      "--------------------------------------------------------------------------------\n",
      "Predicted probability: 0.7155815958976746\n",
      "True label: 0\n",
      "Text: There's been a vogue for the past few years for often-as-not ironic zombie-related films, as well as other media incarnations of the flesh- eating resurrected dead. \"Fido\" is a film that's either an attempt to cash in on that, simply a manifestation of it, or both -- and it falls squarely into the category of ironic zombies. The joke here is that we get to see the walking dead in the contrasting context of a broadly stereotyped, squeaky-clean, alternate-history (we are in the wake of a great Zombie War, and the creatures are now being domesticated as slaves) version of a 1950s suburb. <br /><br />It's a moderately funny concept on its own, and enough perhaps for a five-minute comedy sketch, but it can't hold up a feature-film on its own. The joke that rotting corpses for servants are incongruous with this idealized version of a small town is repeated over and over again, and loses all effectiveness. The soundtrack relentlessly plays sunny tunes while zombies cannibalize bystanders. The word \"zombie\" is constantly inserted into an otherwise familiarly homey line for a cheap attempt at a laugh. <br /><br />The very broadness and artificiality of the representation of \"the nineteen fifties\" here can't help but irritate me. It is so stylized, in it evidently \"Pleasantville-\"inspired way, that it is more apparent in waving markers of its 1950s-ness around than actually bearing any resemblance to anything that might have happened between 1950 and 1959. There is something obnoxiously sneering about it, as if the film is bragging emptily and thoughtlessly about how more open, down-to-Earth, and superior the 2000s are. <br /><br />Because the characters are such broad representations of pop-culture 1950s \"types,\" it's difficult to develop much emotional investment in them. Each has a few character traits thrown at him or her -- Helen is obsessed with appearances, and Bill loves golf and his haunted by having had to kill his father -- but they remain quite two-dimensional. Performances within the constraints of this bad writing are fine. The best is Billy Connolly as Fido the zombie, who in the tradition of Boris Karloff in \"Frankenstein\" actually imparts character and sympathy to a lumbering green monster who cannot speak. <br /><br />There are little bits of unsubtle allegory thrown around -- to commodity fetishism, racism, classism, war paranoia, et cetera, but none of it really works on a comprehensive level, and the filmmakers don;t really stick with anything. <br /><br />Unfortunately, this film doesn't really get past sticking with the flimsy joke of \"Look! Zombies in 'Leave it to Beaver!'\" for a good hour- and-a-half.\n",
      "--------------------------------------------------------------------------------\n",
      "Predicted probability: 0.7142670750617981\n",
      "True label: 0\n",
      "Text: One of, if not the worst film to come out of Britain in the 80s. <br /><br />This tawdry tale of a middle aged lecher who 'seduces' two teenage scrubbers who babysit for him and his faux-posh wife has nothing to redeem it.<br /><br />In turns gratuitous, puerile, uncouth and unrealistic, this film plumbs the depths as it fails miserably in its attempts to be funny, provocative, intellectual and controversial. <br /><br />Perhaps the worst thing about this film is the way the strong cast of George Costigan, Michelle Holmes and Siobhan Finneran are completely stitched by such a lame script. It's no surprise that this was the late Andrea Dunbar's only work to make it onto the screen. Complete and utter rubbish on every level.\n",
      "--------------------------------------------------------------------------------\n",
      "Predicted probability: 0.7094950079917908\n",
      "True label: 0\n",
      "Text: I'll keep this short; thanks to Greg for helping me to put this succinctly: Captivity is about a guy who drugs a girl's drink, imprisons and tortures her, then poses as a captive to have sex with her. That is the single twist and punchline of the film. It's torture as slow motion date rape. And, it's not even a good movie. It's not so bad it's good; it's just bad.<br /><br />It should also be mentioned that among critics, there is a \"spoiler code\" that they dare not break, even though some were tempted to on this one because it is so vile. Why NO ONE had the cojones to step up and say, \"this is garbage, and this is why,\" is beyond me.<br /><br />Don't give your money to these poop-peddlers.\n",
      "--------------------------------------------------------------------------------\n",
      "Predicted probability: 0.7055560350418091\n",
      "True label: 0\n",
      "Text: Utterly predictable silly show about a man who has killed his wife by mowing her down when driving and claimed he had blacked out. Why was he still driving a car? Why did he still feel able to drive a car having killed his wife with one? This question has not occurred to the writers. The story then witters on about a psychologist and her failing marriage which is tied into the failing marriage of wife-killing blackout driver. An omniscient mother and one dimensional child are thrown in for good measure, and the whole builds up to a predictable denouement and crashing finale. Are police psychologists so easily taken in? Deadful writing that the actors do their best with, but they are doomed to failure. This is on a par with a Harlequin Romance. Don't waste your time watching this one unless that's what you are aiming for.\n",
      "--------------------------------------------------------------------------------\n",
      "Predicted probability: 0.7055142521858215\n",
      "True label: 1\n",
      "Text: [***POSSIBLE SPOILERS***] This movie's reputation precedes it, so it was with anticipation that I sat down to watch it in letterbox on TCM. What a major disappointment.<br /><br />The cast is superb and the production values are first-rate, but the characters are without depth, the plot is thin, and the whole thing goes on too long. For a movie that deals with alcoholism, family divisions, unfaithfulness, gambling, and sexual repression, the movie is curiously flat, prosaic, lifeless, and cliche-ridden. One example is the portrayal of Frank Hirsch's unfaithfuness: his rather heavy-handed request to his wife to \"go upstairs and relax a bit\" followed by her predictable pleading of a headache, leads - even more predictably - to his evening liaison with his secretary (\"hey Nancy, I've got the blues tonight. Let's go for a drive\"), all according to well-worn formula. We don't feel these are real people, but cardboard cutouts acting in a marionette play. Also, the source of the obvious friction between Frank and Dave Hirsch is never really explored or explained. Dave's infatuation with the on-again/off-again Gwen is inexplicable in light of her fatuous inability to defecate or get off the pot. His subsequent marriage of desperation to the Shirley Maclaine/Ginny character is, from the moment of its being presented to this viewer, anyway, obviously doomed to fail, and it was clear - by the conventions of this type of soap opera - that it could only be resolved by someone being killed. The moment the jealous lover started running around with the gun I started a bet with myself as to who - Dave or Ginny - would get killed. The whole thing was phony with a capital 'P'. <br /><br />Having said that, Maclaine's performance and that of Dean Martin are the standouts here. But on the whole I find the movie's interest to be purely that of a period piece of Hollywood history.\n",
      "--------------------------------------------------------------------------------\n",
      "Predicted probability: 0.6942896842956543\n",
      "True label: 0\n",
      "Text: One of the more 'literate' Lone Stars, with time spent on character development and interaction, dialog and acting business. The opening scene sets the stage (literally) for the personalities of the gambler, Kansas Charlie (Eddy Chandler), and his buddy, John Scott (John Wayne) the rodeo (say Roh-Day-oh) star, both of whom are slightly randy. The film follows their adventures, as they try to best each other in the pursuit of the Mexican Juanita, and later in their pursuit of perky Mary Kornman, who has the inevitable evil brother (though he'd been led astray by the real villain, and wants to repent). And oh, of course, they're being wrongly accused of two crimes and have to serve jail time before escaping and being exonerated at the end.<br /><br />The high point is Scott continually and deliberately ogling Mary's butt in her grocery store, and knocking away the ladder she's standing on so he can catch her and grab her as she falls. It all seems a little contemporary for a 30s western, but it sounds better than it actually is. <br /><br />Sadly, the exciting action elements we find in many other Lone Stars are sorely missing here. No Yakima Canutt. Cheap and bad uses of stock footage of riders falling off horses. No George Hayes. Tedious Stooge-like bi-play between Scott and Charlie, with Charlie swinging at Scott, Scott stomping on his foot and then punching him (repeated two more times!). The skilled Paul Fix is underused. Eddy Chandler himself, here in his big star turn, is not really believable as a randy side kick. The villain looks too old and fat. So does Chandler, who spent his later career in 300 more movies as an uncredited meatloaf. Mary Kornman, of the twenties \"Our Gang\" (see 'Mary, Queen of Tots' 1925) is cute in her scenes with John Wayne, but that's about it for this one. Seeds of a better western lie buried here.<br /><br />P.S. The ultra-short colorized version, which looks good, moves along so fast, it's over if you blink more than once. Thankfully though, the embarrassing scenes with Eddy Chandler have been cut.\n",
      "--------------------------------------------------------------------------------\n",
      "Predicted probability: 0.69367516040802\n",
      "True label: 0\n",
      "Text: If an auteur gives himself 2 credits before the main title and about 15 more credits before the movie starts, and the first shot shows the auteur rolling around on a bed in lycra bike shorts, it won't be a surprise to observe that said auteur has the kind of body that should never be seen in spandex. The kind of look that might be useful to a homosexual aversion therapist.<br /><br />Others have given this thing the dishing it deserves. For me the most pitiable moment came when the trip from LA was signified by a plane landing at what appeared to be LAX; and the return was signified by a shot of a Fedex cargo plane.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "top_idxs = reversed(probabilities[:, 1].sort().indices)[:10]\n",
    "for i in top_idxs:\n",
    "    print(f\"Predicted probability: {probabilities[i,1]}\")\n",
    "    print(f\"True label: {predictions_output.label_ids[i]}\")\n",
    "    print(f\"Text: {dataset['test'][i.item()]['text']}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
