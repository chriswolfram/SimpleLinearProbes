{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import datasets\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GPT-2 tokenizer and model\n",
    "tokenizer = transformers.GPT2Tokenizer.from_pretrained('gpt2', device_map='auto')\n",
    "# Add a padding token to GPT-2 tokenizer (since it doesn't have one by default)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a small subset of the IMDb dataset for binary sentiment classification\n",
    "dataset = datasets.load_dataset('imdb', split='train[:50%]')\n",
    "dataset = dataset.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e84db2c4907f4a7f9c71aa2c6e30b34f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b60e7b13f384d6d8f68eea3cec67b20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Annotate with whether a text contains the word 'alien'\n",
    "def set_word_label(example):\n",
    "    if 'alien' in example['text'].lower():\n",
    "        example['label'] = 1\n",
    "    else:\n",
    "        example['label'] = 0\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(set_word_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c424912fb8bf44aabf2739bbdfd190ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "653c11e09561478abd670c4a38cd1810",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'], padding='max_length', truncation=True, max_length=128\n",
    "    )\n",
    "\n",
    "# Apply the tokenization\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Remove unnecessary columns and set format for PyTorch\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['text'])\n",
    "tokenized_datasets.set_format('torch')\n",
    "\n",
    "# Split into train and evaluation datasets\n",
    "train_dataset = tokenized_datasets['train']\n",
    "eval_dataset = tokenized_datasets['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom model with GPT-2 as feature extractor and a linear classifier on top\n",
    "class GPT2ForClassification(torch.nn.Module):\n",
    "    def __init__(self, gpt2, num_labels):\n",
    "        super(GPT2ForClassification, self).__init__()\n",
    "        self.gpt2 = gpt2\n",
    "        # Freeze GPT-2 parameters\n",
    "        for param in self.gpt2.parameters():\n",
    "            param.requires_grad = False\n",
    "        # Linear classifier\n",
    "        self.classifier = torch.nn.Linear(self.gpt2.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        # Get hidden states from GPT-2\n",
    "        outputs = self.gpt2(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Use the hidden state of the last token for classification\n",
    "        last_token_indices = attention_mask.sum(dim=1) - 1\n",
    "        pooled_output = outputs.last_hidden_state[torch.arange(input_ids.size(0)), last_token_indices]\n",
    "        logits = self.classifier(pooled_output)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Compute loss\n",
    "            loss_fct = torch.nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.classifier.out_features), labels.view(-1))\n",
    "        return {'loss': loss, 'logits': logits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GPT-2\n",
    "gpt2 = transformers.GPT2Model.from_pretrained('gpt2', device_map='auto', torch_dtype='auto')\n",
    "\n",
    "# Initialize the model\n",
    "num_labels = 2  # Binary classification\n",
    "model = GPT2ForClassification(gpt2, num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments to train for only 1 epoch\n",
    "training_args = transformers.TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    eval_strategy=\"epoch\",\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute evaluation metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = torch.argmax(torch.from_numpy(logits), dim=-1).numpy()\n",
    "    accuracy = sklearn.metrics.accuracy_score(labels, predictions)\n",
    "    return {'accuracy': accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Trainer\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c18152fcfe246a9818712f9b89c4025",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.7486, 'grad_norm': 359.2467956542969, 'learning_rate': 4.92e-05, 'epoch': 0.02}\n",
      "{'loss': 3.9351, 'grad_norm': 326.4512939453125, 'learning_rate': 4.8400000000000004e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4848, 'grad_norm': 370.4857482910156, 'learning_rate': 4.76e-05, 'epoch': 0.05}\n",
      "{'loss': 3.1725, 'grad_norm': 352.8597717285156, 'learning_rate': 4.6800000000000006e-05, 'epoch': 0.06}\n",
      "{'loss': 2.5359, 'grad_norm': 293.3836669921875, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.08}\n",
      "{'loss': 2.2321, 'grad_norm': 272.8007507324219, 'learning_rate': 4.52e-05, 'epoch': 0.1}\n",
      "{'loss': 1.7606, 'grad_norm': 291.4390869140625, 'learning_rate': 4.44e-05, 'epoch': 0.11}\n",
      "{'loss': 1.6087, 'grad_norm': 269.3735046386719, 'learning_rate': 4.36e-05, 'epoch': 0.13}\n",
      "{'loss': 1.2773, 'grad_norm': 222.04600524902344, 'learning_rate': 4.2800000000000004e-05, 'epoch': 0.14}\n",
      "{'loss': 0.9613, 'grad_norm': 178.4942626953125, 'learning_rate': 4.2e-05, 'epoch': 0.16}\n",
      "{'loss': 0.7262, 'grad_norm': 165.3730926513672, 'learning_rate': 4.12e-05, 'epoch': 0.18}\n",
      "{'loss': 0.5361, 'grad_norm': 86.97716522216797, 'learning_rate': 4.0400000000000006e-05, 'epoch': 0.19}\n",
      "{'loss': 0.4491, 'grad_norm': 67.50079345703125, 'learning_rate': 3.960000000000001e-05, 'epoch': 0.21}\n",
      "{'loss': 0.3522, 'grad_norm': 86.81388854980469, 'learning_rate': 3.88e-05, 'epoch': 0.22}\n",
      "{'loss': 0.3035, 'grad_norm': 58.9298210144043, 'learning_rate': 3.8e-05, 'epoch': 0.24}\n",
      "{'loss': 0.1512, 'grad_norm': 44.993492126464844, 'learning_rate': 3.72e-05, 'epoch': 0.26}\n",
      "{'loss': 0.1986, 'grad_norm': 37.47199630737305, 'learning_rate': 3.6400000000000004e-05, 'epoch': 0.27}\n",
      "{'loss': 0.128, 'grad_norm': 14.13881778717041, 'learning_rate': 3.56e-05, 'epoch': 0.29}\n",
      "{'loss': 0.1415, 'grad_norm': 20.622814178466797, 'learning_rate': 3.48e-05, 'epoch': 0.3}\n",
      "{'loss': 0.1, 'grad_norm': 5.066899299621582, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.32}\n",
      "{'loss': 0.0785, 'grad_norm': 10.897958755493164, 'learning_rate': 3.32e-05, 'epoch': 0.34}\n",
      "{'loss': 0.1145, 'grad_norm': 6.67873477935791, 'learning_rate': 3.24e-05, 'epoch': 0.35}\n",
      "{'loss': 0.092, 'grad_norm': 6.473783016204834, 'learning_rate': 3.16e-05, 'epoch': 0.37}\n",
      "{'loss': 0.1239, 'grad_norm': 9.765583992004395, 'learning_rate': 3.08e-05, 'epoch': 0.38}\n",
      "{'loss': 0.1532, 'grad_norm': 13.325518608093262, 'learning_rate': 3e-05, 'epoch': 0.4}\n",
      "{'loss': 0.1871, 'grad_norm': 10.460067749023438, 'learning_rate': 2.9199999999999998e-05, 'epoch': 0.42}\n",
      "{'loss': 0.1187, 'grad_norm': 8.61307144165039, 'learning_rate': 2.84e-05, 'epoch': 0.43}\n",
      "{'loss': 0.1559, 'grad_norm': 9.386993408203125, 'learning_rate': 2.7600000000000003e-05, 'epoch': 0.45}\n",
      "{'loss': 0.1676, 'grad_norm': 5.697088718414307, 'learning_rate': 2.6800000000000004e-05, 'epoch': 0.46}\n",
      "{'loss': 0.1053, 'grad_norm': 3.3213114738464355, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.48}\n",
      "{'loss': 0.0978, 'grad_norm': 4.22200870513916, 'learning_rate': 2.5200000000000003e-05, 'epoch': 0.5}\n",
      "{'loss': 0.2448, 'grad_norm': 2.108255624771118, 'learning_rate': 2.44e-05, 'epoch': 0.51}\n",
      "{'loss': 0.1155, 'grad_norm': 2.7031328678131104, 'learning_rate': 2.36e-05, 'epoch': 0.53}\n",
      "{'loss': 0.1534, 'grad_norm': 2.1509861946105957, 'learning_rate': 2.2800000000000002e-05, 'epoch': 0.54}\n",
      "{'loss': 0.1615, 'grad_norm': 3.894321918487549, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.56}\n",
      "{'loss': 0.1573, 'grad_norm': 3.4244534969329834, 'learning_rate': 2.12e-05, 'epoch': 0.58}\n",
      "{'loss': 0.1331, 'grad_norm': 27.645584106445312, 'learning_rate': 2.04e-05, 'epoch': 0.59}\n",
      "{'loss': 0.2488, 'grad_norm': 18.50890350341797, 'learning_rate': 1.9600000000000002e-05, 'epoch': 0.61}\n",
      "{'loss': 0.0803, 'grad_norm': 4.368836879730225, 'learning_rate': 1.88e-05, 'epoch': 0.62}\n",
      "{'loss': 0.1816, 'grad_norm': 19.289371490478516, 'learning_rate': 1.8e-05, 'epoch': 0.64}\n",
      "{'loss': 0.1112, 'grad_norm': 4.119208335876465, 'learning_rate': 1.7199999999999998e-05, 'epoch': 0.66}\n",
      "{'loss': 0.2292, 'grad_norm': 4.0085835456848145, 'learning_rate': 1.6400000000000002e-05, 'epoch': 0.67}\n",
      "{'loss': 0.0701, 'grad_norm': 5.174537658691406, 'learning_rate': 1.56e-05, 'epoch': 0.69}\n",
      "{'loss': 0.1477, 'grad_norm': 4.071195125579834, 'learning_rate': 1.48e-05, 'epoch': 0.7}\n",
      "{'loss': 0.1879, 'grad_norm': 4.823503494262695, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.72}\n",
      "{'loss': 0.2012, 'grad_norm': 38.388816833496094, 'learning_rate': 1.32e-05, 'epoch': 0.74}\n",
      "{'loss': 0.1893, 'grad_norm': 4.5572638511657715, 'learning_rate': 1.24e-05, 'epoch': 0.75}\n",
      "{'loss': 0.1236, 'grad_norm': 4.558894634246826, 'learning_rate': 1.16e-05, 'epoch': 0.77}\n",
      "{'loss': 0.3405, 'grad_norm': 52.68434143066406, 'learning_rate': 1.08e-05, 'epoch': 0.78}\n",
      "{'loss': 0.23, 'grad_norm': 28.0399227142334, 'learning_rate': 1e-05, 'epoch': 0.8}\n",
      "{'loss': 0.1199, 'grad_norm': 2.4258389472961426, 'learning_rate': 9.2e-06, 'epoch': 0.82}\n",
      "{'loss': 0.1347, 'grad_norm': 20.484798431396484, 'learning_rate': 8.400000000000001e-06, 'epoch': 0.83}\n",
      "{'loss': 0.0958, 'grad_norm': 3.0135107040405273, 'learning_rate': 7.6e-06, 'epoch': 0.85}\n",
      "{'loss': 0.1428, 'grad_norm': 11.31156063079834, 'learning_rate': 6.800000000000001e-06, 'epoch': 0.86}\n",
      "{'loss': 0.207, 'grad_norm': 2.5800487995147705, 'learning_rate': 6e-06, 'epoch': 0.88}\n",
      "{'loss': 0.0837, 'grad_norm': 2.763557195663452, 'learning_rate': 5.2e-06, 'epoch': 0.9}\n",
      "{'loss': 0.1746, 'grad_norm': 5.460850715637207, 'learning_rate': 4.4e-06, 'epoch': 0.91}\n",
      "{'loss': 0.1293, 'grad_norm': 1.8103322982788086, 'learning_rate': 3.6e-06, 'epoch': 0.93}\n",
      "{'loss': 0.1632, 'grad_norm': 48.705509185791016, 'learning_rate': 2.8000000000000003e-06, 'epoch': 0.94}\n",
      "{'loss': 0.1427, 'grad_norm': 14.12005615234375, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.96}\n",
      "{'loss': 0.121, 'grad_norm': 1.700151801109314, 'learning_rate': 1.2000000000000002e-06, 'epoch': 0.98}\n",
      "{'loss': 0.1339, 'grad_norm': 3.0669209957122803, 'learning_rate': 4.0000000000000003e-07, 'epoch': 0.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d42ef8a0dff40c7901fbfa6ebd7e14e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1842571198940277, 'eval_accuracy': 0.9596, 'eval_runtime': 12.3402, 'eval_samples_per_second': 202.591, 'eval_steps_per_second': 12.723, 'epoch': 1.0}\n",
      "{'train_runtime': 62.4355, 'train_samples_per_second': 160.165, 'train_steps_per_second': 10.01, 'train_loss': 0.5643834292411805, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=625, training_loss=0.5643834292411805, metrics={'train_runtime': 62.4355, 'train_samples_per_second': 160.165, 'train_steps_per_second': 10.01, 'total_flos': 0.0, 'train_loss': 0.5643834292411805, 'epoch': 1.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50aa995490b64aceadf1980b1728b8b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.1842571198940277,\n",
       " 'eval_accuracy': 0.9596,\n",
       " 'eval_runtime': 12.2539,\n",
       " 'eval_samples_per_second': 204.016,\n",
       " 'eval_steps_per_second': 12.812,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bee280573974d13bd09019ef0a8ef91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use the trainer to make predictions on the evaluation dataset\n",
    "predictions_output = trainer.predict(eval_dataset)\n",
    "probabilities = torch.nn.functional.softmax(torch.tensor(predictions_output.predictions), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  37,   70,  112,  285,  297,  339,  346,  367,  381,  488,  498,\n",
       "         587,  599,  820,  931, 1036, 1065, 1149, 1179, 1296, 1383, 1494,\n",
       "        1549, 1558, 1589, 1744, 1805, 1867, 1914, 1922, 1939, 1970, 1980,\n",
       "        2047, 2074, 2111, 2150, 2227, 2228, 2245, 2476]),)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_output.predictions.argmax(axis=-1).nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   5,    9,  172,  187,  194,  235,  243,  260,  275,  379,  412,\n",
       "         424,  444,  445,  473,  485,  507,  527,  539,  705,  711,  802,\n",
       "         850,  905,  913,  942, 1047, 1048, 1078, 1102, 1104, 1265, 1353,\n",
       "        1401, 1407, 1443, 1507, 1547, 1679, 1874, 1910, 1924, 1928, 1941,\n",
       "        1988, 1998, 2002, 2023, 2046, 2059, 2080, 2094, 2151, 2187, 2189,\n",
       "        2203, 2377, 2380, 2411, 2447]),)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_output.label_ids.nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(60)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(dataset['test']['label']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted probability: 0.9996633529663086\n",
      "True label: 0\n",
      "Text: I loved \"Anchorman; The Legend of Ron Burgundy\" and hoped this would be just as funny, but alas, it wasn't. Some bits are excellent though. I thought the sports guy, Champ Kind, professing his love for Ron Burgundy in the car filled with the other members of the news team was hilarious. Everyone is ignoring him and he just gets louder and louder and finally kisses Burgundy which doesn't get acknowledged either. But on the whole the story doesn't gel. It's a noble attempt, however, to salvage the unused bits from the first movie, including an entire plot about some pretty benign would-be domestic terrorists called \"The Alarm Clock.\" Maya Rudolph of Saturday Night Live is one of the members and has a couple of funny lines, but basically this unused plot line has good reason to be unused in the first movie. The extras on this disk are pretty good, with the best two being the filmed rehearsals featuring lots of improv comedy, and the faux commentary with Will Ferrell and an \"exec producer\" who Ferrell discovers early on was not even a part of the movie in any way, shape or form. Turns out he's \"just a guy\" who walked in the side door of the recording studio and pretended to be a producer. It's some pretty funny stuff though and goes on for about 10 or 15 minutes\n",
      "--------------------------------------------------------------------------------\n",
      "Predicted probability: 0.9979338645935059\n",
      "True label: 0\n",
      "Text: OVERALL PERFORMANCE :- At last the long waiting AAG hits the screens. Unfortunately, it couldn't set progressive fire in the audience. The first best thing to talk about the movie is The idea of remaking the mighty SHOLAY. And Varma made a nice choice of changing the total backdrop of the movie. If he repeated the same Ramghad backdrop, people will again say there is nothing new in this. Different background is appreciative but the way he presented it is not worthy. Right from the start of his career with SIVA in Telugu, he had been using the same lighting and kind of background. I seriously dunno this guy Varma considers about lighting or not or may be he has no other lighting technique other than like gordon willis GODFATHER. It's all DUTCH DUTCH DUTCH DUTCH. Why would some body use so many Dutch angles and extreme closeup shots!!!!!!! The shot division is lame. Characters couldn't carry an emotion, performances are not to their mark, Storytelling is worse, Background is really really terrible.<br /><br />Babban:- Amitabhz been over prioritized to his job. VARMA produced great villains like Bikumatre, Bhavtakur Das, Mallik Bhai but this time he failed in carving the all time best characters of Hindi Cinema. There's no comparison of Gabbar with Babban. Babban is a more psycho rather than a villain, still he has a soft corner for his brother ( It's a gift in this movie). Amitabhz performance is not to his mark. His appearance itself is pathetic. The scar on his nose, symbolizes forgotten villains of black and white cinema. What ever they worked on Babban is not successful. Babban is no comparison with Gabbar.<br /><br />Narsimha:- The first best thing about this character is not to put audience in suspense about his hands. If varma did that , it would be like teaching ABCD to a Bachelor degree holder. Itz good he opened the secret early. But the flashback is pathetic. Varma couldn't use a great actor like mohanlal to his mark.<br /><br />Durga:- The only character with betterment. This character has been improved with satisfactory changes and was used according to the story.<br /><br />Heroo, Raj, Ghunguroo:- No body bothers or at least considers these character. The utter failure of movie starts when director could not work on the close friendship between our heroz. These characters carry nothing to this movie.<br /><br />RAMGOPALVARMA:- His quality is degrading, diminishing. AAG totally can be treated as a C grade movie. Sholay is a fire of revenge, problem of a town, meaning for true friendship and highly appreciated nuisance and fun by Dharmendra. AAG never carried an emotion with its characters. Storytelling is too weak that it could not make audience feel sympathy for the characters. Don't compare AAG with sholay, still u will not like it.<br /><br />If you dare watch this movie. You will be burnt alive in RAMGOPAL VARMA KI AAG\n",
      "--------------------------------------------------------------------------------\n",
      "Predicted probability: 0.9979336261749268\n",
      "True label: 0\n",
      "Text: [I saw this movie once late on a public tv station, so I don't know if it's on video or not.]<br /><br />This is one of the \"Baby Burlesks\" (sic) that Shirley Temple did in the early 1930s. It is hard to believe that anyone would let their daughter be in this racy little film which today might just be considered this side of \"kiddie porn\".<br /><br />Shirley Temple stars in a cast which probably has an average age of 5. They are all in diapers, and are in a saloon which serves milk instead of alcohol. The \"cash\" is in the form of lollipops.<br /><br />Shirley playing a \"femme fatale\" sashays up to the bar and talks to soldiers who make suggestive comments about her (!). But Shirley doesn't need really their lollipops/cash because her purse is full of ones from other \"men\".<br /><br />Meanwhile a little black boy does a suggestive dance on a nearby table (!).<br /><br />What a strange film . . . infants using racy dialogue playing adult roles in a saloon. Who thought up this stuff any way?\n",
      "--------------------------------------------------------------------------------\n",
      "Predicted probability: 0.9977205395698547\n",
      "True label: 0\n",
      "Text: Well, there you have it, another disillusion on my account. Two, actually! First of all, even though I like to think of myself that I know a little something about 70's euro-exploitation and its most prolific contributors, I never heard about Joseph W. Sarno before. Here's a guy who made over seventy rancid and cult-laden exploitation movies and I haven't seen a single one! How? Why? What happened here? Secondly, and even worse, just when you think to have found a new source for obscure cult movies, that director's most famous and supposed \"masterpiece\" turns out to be an irredeemably dull and irritating film. Admittedly, lesbian vampire movies form a pretty insignificant sub genre as a whole, but some of them bath in ominous atmosphere and curious sensuality (like José Larraz' \"Vampyres\" or Harry Kümmel's \"Daughters of Darkness\"). Joseph Sarno's film has nothing to offer, except copious amounts of gratuitous nudity and even that becomes boring rather quickly. The events take place in a secluded old castle, hidden deep in the German mountains, where five centuries ago lived a malicious and bloodthirsty (literally) baroness. Her loyal disciples still throw naked dance parties in the castle's catacombs, which are lit by penis-shaped candlesAUCH, and hope to resurrect the baroness any time soon now. Suddenly (don't even ask how) the castle is full of young and sexy female guests, so even more erotic rites ensue. Sounds delicious and entertaining enough, but \"The Devil's Plaything\" contains a massive number of sequences where literally nothing happens and where the cast members' ignorant facial expressions are simply unendurable! Sarno isn't capable of creating suspense or building a Gothic atmosphere (or maybe he just didn't bother to) and the actresses' capacities restrict themselves to standing in front the camera topless and pull a really pathetic face. Please do yourself a favor: no matter how desperately you strive to see all lurid lesbian-vampire movies of the 70's, this one isn't worth a penny! Even the repertoires of Jess Franco and Jean Rollin are pure art compared to this dud.\n",
      "--------------------------------------------------------------------------------\n",
      "Predicted probability: 0.9954351782798767\n",
      "True label: 0\n",
      "Text: Simon Pegg stars as Sidney Young, a stereotypically clumsy idiot Brit working as a celebrity journalist in this US comedy. After getting a very lucky break he starts work at the highly respected Sharps magazine run by a reliably on form Jeff Bridges in New York. It's more The Devil Wears Prada than Shaun of the Dead. The unlikely love interest is provided by Kirsten Dunst who works well with Pegg for the laughs but they don't exactly set the screen ablaze with their passion.<br /><br />Sidney goes through some emotional challenges while trying to decide if he should forget about his journalistic principles in order to get material in the magazine. Of course he's eventually seduced by the glitz and glamour of the world of celebrities especially the young starlet Sophie Maes (Transformers' Megan Fox). Fans of Shaun of the Dead, Hot Fuzz and Spaced will wonder if Pegg himself ever experienced similar feelings in real life with this film and to an extent Run Fat Boy Run as one of the UK's best comic talents being ruined by the US.<br /><br />All in all this is a forgettable comedy. Please come back to us Simon, we can forgive and forget.\n",
      "--------------------------------------------------------------------------------\n",
      "Predicted probability: 0.9908554553985596\n",
      "True label: 0\n",
      "Text: Despite being a huge fan of Fred Astaire and Ginger Rogers' movies, it wasn't until about 6 years ago that I first saw 'Follow the Fleet'. I knew all the songs from an old Astaire/Rogers record (yes, vinyl) but knew nothing of the plot.<br /><br />Unfortunately, while the songs are catchy and Ginger Rogers' character is sweet and funny, you just can't like 'Bake Baker'. While trying to make up to his longtime partner, he continually sabotages her career. His character doesn't have the usual humour and elan of the other films' Astaire characters.<br /><br />Worth watching for the songs and a great solo tap routine by Ginger Rogers.\n",
      "--------------------------------------------------------------------------------\n",
      "Predicted probability: 0.9903074502944946\n",
      "True label: 0\n",
      "Text: While William Shater can always make me smile in anything he appears in, (and I especially love him as Denny Crane in Boston Legal), well, this show is all about glitz and dancing girls and screaming and jumping up and down.<br /><br />It has none of the intelligence of Millionaire, none of the flair of Deal or No Deal.<br /><br />This show is all about dancing and stupid things to fill in the time.<br /><br />I watched it of course just to check it out. I did watch it for over 45 minutes, then I had to turn it off.<br /><br />The best part of it was William Shatner dancing on the stage. He is a hoot!!! unfortunately, this show WILL NOT MAKE IT.<br /><br />That's a given\n",
      "--------------------------------------------------------------------------------\n",
      "Predicted probability: 0.9884048700332642\n",
      "True label: 0\n",
      "Text: Set in the 70s, \"Seed\" centers around convicted serial killer Max Seed (Will Sanderson), who killed 666 people in 6 years. He is sentenced to death, but in the electric chair he doesn't die, even after being shocked three times.<br /><br />Detective Matt Bishop (Michael Paré) and other officers cover up this secret by burying Seed alive. Seed breaks out and goes after the people who put him in his living coffin.<br /><br />Filmed by the worst director in the world (Uwe Boll), \"Seed\" is nothing more than a snuff film about trying to stretch the envelope of decent society and fails to deliver in any aspect of a storyline. And he said this is based on true events because if a person survives the electric chair after being shocked three times, they will be set free. This is an urban legend, and it would never happen. Much like Boll's other abominations (\"Alone in the Dark\" for one), \"Seed\" is just utterly horrendous.\n",
      "--------------------------------------------------------------------------------\n",
      "Predicted probability: 0.9872032999992371\n",
      "True label: 0\n",
      "Text: How low can someone sink while trying to recapture an old glory? ST:HF will be glad to show you.<br /><br />If you are used to seeing what made for a good Star Trek show, do NOT watch this.<br /><br />The writing is hodge-podge, the actors' portrayals of their characters weak, and most of all, the design work is downright doggy.<br /><br />Like watching strong captains, don't look here! Like the strong Federation attitude? Forget about it here! Starfleet is mocked by ensigns wearing SPIKES in their hair.<br /><br />While a seemingly mentally feeble captain shuffles about and within two minutes of the opening show's credits, Ensign Spikey is attempting to arrange a tryst with an engineer. It just degrades from there. No, not even uniforms match, for goodness sake. They are too small or too big, collars down to their chests, and TNG Seasons One and Two Uniforms mixed in with Season Three and DS9 uniforms. The strict discipline and tradition of any of the originals in lacking in this production down to the treads! The only good thing about this show is its graphics, which seem to improve a bit with each season. OK, I take that back. Who uses CG that inexpertly? The designers of this show.<br /><br />Don't bother with it, it will offend your Star Trek sense, as it did mine. Not even the throw backs to previous shows can save this catastrophe.<br /><br />I wept openly when i watched this, probably because my eyes were bleeding and my head almost ruptured. That bad.\n",
      "--------------------------------------------------------------------------------\n",
      "Predicted probability: 0.9746280312538147\n",
      "True label: 0\n",
      "Text: An OK flick, set in Mexico, about a hit-man (Scott Glenn) who hitches a ride with struggling American writer and his Mexican girlfriend after a hit. He pays them to take him to the border  but things get out of hand.<br /><br />It starts well enough, but quickly struggles and dies.<br /><br />**SPOILER**<br /><br />The eventual relationship twist is badly set up and difficult to believe. An absence of passion, and essentially no reasoning behind her leaving one man for the other, made it ridiculous - and the ending was predictable and dull.<br /><br />**END SPOILER**<br /><br />Harvey Keitel is the US agent on the hit-man's trail, but he seems a little confused as to how boring and slow the script is...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "top_idxs = reversed(probabilities[:, 1].sort().indices)[:10]\n",
    "for i in top_idxs:\n",
    "    print(f\"Predicted probability: {probabilities[i,1]}\")\n",
    "    print(f\"True label: {predictions_output.label_ids[i]}\")\n",
    "    print(f\"Text: {dataset['test'][i.item()]['text']}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
